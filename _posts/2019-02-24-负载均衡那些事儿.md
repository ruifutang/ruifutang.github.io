---
layout:     post
title:      负载均衡那些事儿
subtitle:   nginx or lvs
date:       2019-02-24
author:     唐瑞甫
header-img: img/post-bg-coffee.jpeg
catalog: true
tags: 
    - lb


---  

通常在单机出现性能瓶颈，或者单机无法支撑当前数据量的请求时，会将单机服务器替换为集群服务器来提升系统整体的处理性能。  
  
当再有请求过来时，就需要做出一种合理的选择，如何将请求映射到后端的某一台服务器上面，这时一般就需要增加一个 Load Balancer(下文简称为 LB) 来完成这个需求。  
  
常见的LB可以分为3种：『 DNS LB 』 『 硬件LB 』 『 软件LB 』。  
  
由于日常工作中需要操作 DNS LB 跟 硬件LB(F5,A10,etc) 的机会比较少，下面主要针对『软件LB』这种类型来进行展开。
   
    
  
### Nginx

> Nginx is a web server which can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache.    
  
**Nginx** 在日常生活中普遍能见到，而且用途很广泛，会被拿来作为web服务器，也会被拿来作为http反向代理。  
  
作为LB的话，Nginx支持L7的负载均衡，也就是可以处理http协议，而且Nginx的强大之处还在于配置及其灵活，可以支持用户自定义的插件来实现业务的定制化功能；
  
最简单的 Nginx LB 的配置示例如下:
  
```
http {
    upstream backend {
        server a;
        server b;
        server c backup;
    }

    server {
        listen 80;
        server_name www.example.com;
        
        location / {
            proxy_pass http://backend;
        }
    }
}

```
这样nginx收到请求后会按顺序进行**轮询**将请求转到后台的服务器上。轮询的好处就在于简单，但是如果集群中的机器进行物理配置升级后，就有可能会出现新老机器配置不一致的情况，比如有的机器可能是128核64G，有的可能是256核128G的配置，这样就需要在此基础上进行**加权轮询**  
  
  
```
upstream backend {
    server a weight=1;
    server b weight=2;
    server c backup;
}

```
这样更高配置的机器就会有机会处理到更多得请求，毕竟能者多劳嘛。但是每个请求是不一样的，对性能及资源的消耗也会不一样，这样还是可能会出现某些机器负载很高的情况下还有接收到更大量的请求，这时就需要有更人性化的配置,需要根据机器的负载(cpu负载，io负载，http请求数等等)来进行相应的选择，比如  
  
```
upstream backend {
     least_conn;

     server a;
     server b;
     server c backend;
}
    
```
使用最小连接方法，Nginx将其与每个服务器的当前活动连接数进行比较，并将请求发送到具有最少连接的服务器上。  
  
当然，具体的配置策略因具体业务而异，这里就不一一展开了，感兴趣的话推荐看下[官方文档](https://docs.nginx.com/nginx/)  
  
### LVS

LVS 全称是 Linux Vitual Server。而且 LVS 工作在 L4，相对于 Nginx 这种更高层的 LB ,效率更高。

> Linux Virtual Server (LVS) is load balancing software for Linux kernel–based operating systems.  
  
LVS 的 负载均衡技术是通过 IPVS 模块来实现的，该模块的主要作用是：安装在Director Server上，同时在Director Server上虚拟出一个 Virtual IP(简称 VIP) ，Client 必须通过 VIP 来访问后端服务。对于 Director，VIP 用于接受客户端请求，DIP(Director IP) 用于和 RS 通信。请求首先经过 VIP 到达 LVS，然后由 LVS 从 RSs 中选取一个节点响应用户的请求。   
  
LVS 可以分为三种，分别是「 NAT 」「 DR 」「 TUN 」。
  
#### LVS-NAT

NAT的全称是 Network Address Translation。流程图如下所示。  
  
![nat](/img/image/lvs3.gif)
  
当client请求集群服务时，LVS 会根据相应的选取策略修改请求报文的目标地址为 RS 对应的 IP，然后转发至后端的 RS ，并在响应报文返回至 LVS 后修改其源地址为 VIP，然后发送给client。

比如上图中：  
  
Client 对应的 SRC IP 为 202.100.1.2 ，PORT 为 3765  
  
LVS 对应的 VIP 为 202.103.106.5 ，PORT 为 80  
  
RS1 对应的 DEST IP 为 172.16.0.2 ， PORT为 8066

这样：

从 Client 到 LVS 的 请求报文 为：  
SOURCE  	202.100.1.2:3756	  DEST	  202.103.106.5:80  
  
从 LVS 到 RS1 的 请求报文 为：  
SOURCE  	202.100.1.2:3756	  DEST	  172.16.0.2:8066  
  
从 RS1 到 LVS 的 请求报文 为：  
SOURCE  	172.16.0.2:8066	  DEST	  202.100.1.2:3756  
  
从 LVS 到 Client 的 请求报文 为：  
SOURCE  	202.103.106.5:80	  DEST	  202.100.1.2:3756  

可以看到，LVS-NAT模式  
**优点**：LVS 跟真实服务器可以在不同网段中，网络架构非常灵活，并且支持端口映射。  
**缺点**：请求跟响应的所有流量都要经过 LVS 调度服务器，会使 LVS 很容易成为性能瓶颈。  
  
#### LVS-DR  
DR 的全称是 Direct Routing。流程图如下所示。  
  
![dr](/img/image/lvs1.gif)  
   
与 LVS-NAT 模式不同的是，LVS-DR 模式下 LVS 和 RS 处在同一网络中，且 VIP 地址由 RS 和 LVS 共享(注意需要禁止 RS 响应 VIP 的 ARP 请求)。LVS 只接收 Client 发来的请求并将请求转发给后端服务器，后端服务器处理请求后直接发送响应给 Client，而不用再次经过 LVS。  
  
LVS 需要将 segment 的 MAC 地址修改为 RS 的 MAC 地址，该数据包就会被转发到相应的RS处理，此时的源 IP 和目的 IP 都没变。RS 收到 LVS 转发来的包时，发现MAC是自己的，IP也是自己的，于是这个包被合法地接受。当 RS 返回响应时，只要直接向 Client IP 返回即可，不再经过 LVS。 
  
可以看到，LVS-DR模式  
**优点**：LVS 只处理请求的直接路由转发，所有响应结果由真实服务器处理并直接将响应返回给 Client，对于并发性能有一定的提升。  
**缺点**：LVS 跟所有 RS 在同一个网段中，不能实现跨网段应用，也不支持端口映射。  
  
#### LVS-TUN
TUN 指的是 IP Tunneling。流程图如下所示。  
  
![tun](/img/image/lvs2.gif)  
  
LVS 把请求的报文通过 IP 隧道转发到 RS。RS将响应处理后的数据直接返回给客户端。同样，LVS 只处理请求报文。TUN模式下 LVS 不修改请求报文的IP首部（源IP为CIP，目标IP为VIP），而是在原IP报文之外再封装一个IP首部（源IP是DIP，目标IP是RIP），RS 将请求报文解开后处理请求，然后直接将响应发送给客户端（源IP是VIP，目标IP是CIP）。  
  
可以看到，LVS-TUN模式  
**优点**：LVS 和 RS 之间是通过IP 隧道来进行转发，所以两者可以存在于不同的网段中，提高网络架构的灵活性。  
**缺点**：RIP，DIP，VIP 都是公网地址，且不支持端口映射。  
  
### 拓展延伸
关于 LVS 的负载均衡算法其实跟 Nginx 是类似的，也包含了多种策略可进行配置，具体就没有再一一介绍了。具体可以参考[这篇文章](http://www.linuxvirtualserver.org/docs/scheduling.html)







---
  By 唐瑞甫
  2019-02-24

