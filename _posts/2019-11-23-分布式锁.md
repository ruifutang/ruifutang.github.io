---
layout:     post  
title:      热点 key 穿透缓存了怎么办     
subtitle:   分布式锁怎么锁    
date:       2019-11-23    
author:     唐瑞甫  
header-img: img/post-bg-coffee.jpeg  
catalog: true  
tags:  
    - redis  
    - zookeeper  

---  

上篇文章最后留了一个问题，也就是热点 key 所导致的瞬时流量泛洪的问题。要想解决这个问题，思路就是要避免热点 key 失效。这篇文章专门来聊一聊，如何避免热点 key 所导致的缓存击穿的问题。  
  
### 缓存不过期  
  
这是最容易想到的方案，实现起来并没有想象中的那般容易。由于缓存不过期，所以很容易出现「内存占满」的情况，这时如果没有指定内存淘汰策略，则会出现缓存无法写入的情况，导致异常发生。比如 redis 默认的[内存淘汰策略](https://redis.io/topics/lru-cache)在内存占满的情况下写入会直接报错。  
  
通常有两种思路来实现缓存不过期。  
  
第一种就是直接将所有 key 设置为不过期，这种方案比较适用于缓存诸如全局配置这种数据。如果是那种数据量非常大又需要频繁写入的类型，则意味着在内存占满之后需要频繁地删除数据并写入数据，会影响性能。  
  
第二种就需要「实时监控」来发现出热点 key，然后及时地将这些 key 值设置为不过期，这样既提高内存的使用效率，又不会影响写入性能。唯一需要解决的问题就是，如何发现热点 key。  
  
#### 发现热点key   

对于如何发现热点 key 的问题，这里以 redis 为例，提供一些思路以供参考。  

1. 事先进行预判
  
  比如现在要做一个秒杀活动，那么这些商品对应的 key 就会成为热点 key。这个方案的缺点也很明显，就是并非所有业务都能进行预判，只适用特定的场景。
  
2. 在客户端进行收集
  
  在调用方的代码中对 redis 操作进行统计，可以得到针对每个 key 的访问次数。数据统计的方式有很多种，可以在内存中直接累加，也可以上报数据给外部依赖的系统。该方案的缺点就是对调用方代码造成入侵。
  
3. 在 Proxy 层做收集
 
  如果系统架构中有专门的中间层做代理去操作 redis ，那么就可以在 proxy 里面去做统计。这个方案跟 1 类似，缺点就是只适合有 proxy 的系统架构。  
  
4. 用 redis 自带命令

  4.1 monitor 命令。但是该命令在高并发的条件下，有内存增暴增的隐患，还会降低 redis 的性能。

  4.2 hotkeys 参数。但是该参数在执行的时候，如果key比较多，执行起来比较慢。
  
  
孰优孰劣，只有放到实际的场景里比较，才会有意义。  
  
对于解决热点 key 击穿这个问题，还有一种实际中更常用的解决方案 -- 「分布式锁」。  
  
### 分布式锁  
  
首先，什么是锁？  
  
简单说，锁是实现「**多线程**」同时访问同一资源，保证同一时刻只有一个线程可访问共享资源所做的一种标记。  
  
而「分布式锁」，则是指「**分布式环境**」下，系统部署在多个机器中，实现分布式「**多进程**」互斥的一种锁。 

在热点 key 的场景中，当 key 失效时，通过引入分布式锁可以保证 DB 同一时间只能被一个进程访问，这样也能解决缓存击穿的问题。  
  
有三种常见的方法来实现分布式锁：  
  
- 基于 DB 实现分布式锁  
- 基于 Redis 实现分布式锁
- 基于 ZooKeeper 实现分布式锁  
  
#### 基于 DB 实现  
  
「如果多个请求同时提交，DB 会保证只有一个操作可以成功。」  
  

可以利用 DB 的这一特性来实现分布式锁。创建一张锁表，当我们要锁住某个资源时，就在该表中增加一条记录，想要释放锁的时候就删除这条记录。  
  
```sql
CREATE TABLE `t_distributed_lock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `key` varchar(64) NOT NULL DEFAULT '' COMMENT '缓存 key 值',
  `modify_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_key` (`key`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='分布式锁表';
```  
可以这样进行加锁：  
  
```
insert into t_distributed_lock set key = 'tokyo_hot';

```  
  
可以这样进行解锁：  
  
```
delete from t_distributed_lock key = 'tokyo_hot' limit 1;

```  
  
这个方案会有以下几点需要注意：  
  
1. 单节点问题。由于强依赖 DB 的可用性，一旦 DB 挂掉，会导致业务系统不可用。  
    
	这里可以考虑主备/主从架构，一旦主节点挂掉的话就将流量切换到备机上。  
	
2. 宕机死锁问题。DB 锁没有失效时间，未获得锁的进程只能一直等待已获得锁的进程主动释放锁。一旦已获得锁的进程挂掉或者解锁操作失败，会导致锁记录一直存在数据库中，其他进程无法获得锁。
  
  可以启用一个后台线程做轮训，每隔一定时间把数据库中的超时数据清理一遍，例如当前时间减去 modify_time 超过 1 分钟的记录直接删除。  
   
当然也可以利用数据库本身的排它锁来实现分布式锁。 
  
注意这里需要将自动提交关闭并在 DB 中事先插入一条记录。
  
```
set autocommit = 0；  
  
insert into t_distributed_lock set key = 'tokyo_hot';  

```  
  
加锁就可以这样  
  
```  
begin;  
  
select id from t_distributed_lock where key = 'tokyo_hot' for update;  

```

解锁就可以这样  
  
```
commit;  

```

使用这种方式，宕机之后 DB 会自己把锁释放掉。  
  
不过这个方案有几个点需要特别注意
  
1. MySql会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。这样将极大影响到整个业务的吞吐量。  
  
  这里可以使用 「force index」来强制使用索引来避免全表扫描。  
    
  ```
  select id from t_distributed_lock force index(uidx_key) where key = 'tokyo_hot' for update;  
  
  ```
  
2. for update 使用排他锁来进行加锁操作，如果长时间不提交，就会占用 DB 连接。一旦类似的连接变多了，就可能把连接池撑爆。  
  
  设置 Transaction 超时时间，避免大量连接占用连接池。
    
  ```
  set innodb_lock_wait_timeout = 10;  
  
  ```  
  注意，这里需要打开 innodb_rollback_on_timeout，而且需要在 mysqld 服务启动的时候指定  
  
  ```  
  ./mysqld_safe --user=mysql --innodb_use_native_aio=0 --datadir=/data/mysqlData --innodb_rollback_on_timeout=ON  
  
  ```
  否则超时只会回滚 sql 而不是 transaction。  
  
  > InnoDB rolls back only the last statement on a transaction timeout by default. If innodb_rollback_on_timeout is specified, a transaction timeout causes InnoDB to abort and roll back the entire transaction (the same behavior as before MySQL 5.0.13). This variable was added in MySQL 5.0.32.
  
  
  然后设置竞争锁的过程中为非阻塞，抢不到锁不会等待直接返回。  
    
  ```  
  select id from t_distributed_lock where key = 'tokyo_hot' for update NOWAIT;   
  
  ```  
  
总得来说，因为 DB 需要落磁盘，频繁读取 DB 会增大 IO，因此这种分布式锁适用于并发量低，对性能要求低的场景。



#### 基于 Redis 实现

可以使用 [setnx(key, value)](https://redis.io/commands/setnx) 函数来实现分布式锁。其中 key 表示锁 id，value 表示 currentTime + timeOut，表示当前时间 + 超时时间。当持有锁的时间超过设置的 timeout 时，锁会被系统释放。  
  
```
SETNX lock.foo <current Unix time + lock timeout + 1>  

```
  
setnx 函数的返回值只有 0 跟 1 两种情况，返回 1 则表示获取锁，返回 0 则表示没有获取到锁，但是不会阻塞。当 setnx 返回 0时，可以通过 GETSET 来判断当前锁是否已经过期。  
   
 ```
 GETSET lock.foo <current Unix timestamp + lock timeout + 1>
 ```
不过目前 Redis 并不推荐这个方案来实现分布式锁。  
  
> This pattern is discouraged in favor of the Redlock algorithm which is only a bit more complex to implement, but offers better guarantees and is fault tolerant.  
  
Redis 专门实现了这个 [Redlock 算法](https://redis.io/topics/distlock)，并且提供了各个语言的版本直接可以使用，比如 [JAVA 版本](https://github.com/redisson/redisson) 和 [ C++ 版本](https://github.com/jacket-code/redlock-cpp) 等。链接中均有详细的描述及分析，值得深入学习，这里就不展开了。
  
总的来说，相比于 DB 实现的方案，Redis 性能更好。数据被存放在内存而不是磁盘，避免了频繁的 IO 操作。而且 Redis 可以支持集群部署，解决了单点故障的问题。再加上 Redis 官方开源了各语言版本的实现方案，实际中如果需要用到分布式锁的场景，Redis 是个不错的选择。  
  
#### 基于 Zookeeper 实现  
  
> ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.
  
ZooKeeper 中的节点 znode 具有以下几点性质：  
  
1. 有序节点：可以创建子节点时并且指明有序，那么 ZooKeeper 在生成子节点时会根据当前的子节点数量自动添加整数序号。
  
2. 临时节点：可以建立一个临时节点，在会话结束或者会话「超时」后，zookeeper会自动删除该节点。  
  
3. 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，ZooKeeper 会通知客户端。当前有如下四种事件会进行通知：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。  
  
可以基于「**临时有序节点**」可以实现分布式锁。  
  
1. 在根节点 /lock 下创建临时有序的子节点，第一个 client 对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。  
  
2. client 获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则代表获得锁，否则监听根节点下所有子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；
  
3. 删除子节点则代表释放锁。  
  
这里有个问题，比如 a 对应子节点为编号为 0(方便演示简化了编号)，b对应子节点编号为 1，b 获取子节点列表时发现自己不是序号最小的，但是在设置监听器前 a 删除了编号为 0 的子节点，那这样 b 设置的监听器会因为接受不到变更通知而一直阻塞在等待状态中。  
  
不过 zookeeper 提供的 API 中设置监听器的操作与读操作是「**原子**」执行的，所以这个问题是不存在的。  
  
如果获得锁的 client 释放锁时，剩下所有的 client 都会被唤醒，就会出现「**惊群效应**」类似的现象，此时 zookeeper 需要通知其余所有的 client，可能会阻塞其他的操作。  
  
所以每次释放锁时只唤醒剩下节点中的编号最小的节点对应的 client，在设置事件监听时，每个 client 应该对刚好在它之前的子节点设置事件监听，编号为 2 的 client 监听编号为 1 的子节点删除消息，编号为 1 的监听序号为 0 的子节点删除消息。  
  
于是可以对上述流程中的第 2 步进行优化。  
  
client 获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则代表获得锁，否则**监听刚好在自己之前一位的子节点删除消息**，获得子节点变更通知后重复此步骤直至获得锁。  
  
利用 zookeeper 实现分布式锁，优点就是可靠性高，扩展性好，不会有单点故障。缺点就是需要专门引入 zookeeper 组件，实现成本较高。不过这里可以考虑 [Curator](http://curator.apache.org/index.html)  
  
> Curator is a Java/JVM client library for Apache ZooKeeper, a distributed coordination service. It includes a highlevel API framework and utilities to make using Apache ZooKeeper much easier and more reliable.
  
具体就不做演示了，感兴趣的可以自己研究。  
  
### 方案对比  
  
通常来讲，缓存不过期的方案适用于全局配置，静态数据这类的场景，而在大规模的场景中，分布式锁可能会更实用。  
  
用 DB 实现分布式锁的方案适用于并发量低且对性能要求不高的场景， Redis 的方案适用于高并发并且对性能要求高的场景，而 zookeeper 除了对性能要求极高的场景不适用之外其他场景都能适用。  
  
但是不要忘了，具体场景具体分析。  
  
 
--「天道酬勤 地道酬善」









 
---
  By 唐瑞甫  
  2019-11-23

